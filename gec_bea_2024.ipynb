{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**This notebook shows an encoder-decoder model for Grammatical Error Correction. The following notebook was based on [Ben Trevettâ€™s seq2seq tutorial](https://github.com/bentrevett/pytorch-seq2seq/). The main modification is to adapt the model from machine translation to grammatical error correction.**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change current working directory to Google drive to access necessary files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Importing necessary libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = r\"/content/gdrive/My Drive/Colab Notebooks/gec_bea_2024\"\n",
    "os.chdir(PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customized preprocessor\n",
    "\n",
    "m2_data_preprocessor.py contains a customized class to preprocess m2 files from the BEA 2019 challenge."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from m2_data_preprocessor import DataPreprocessor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Installing and importing necessary libraries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install torchtext==0.17.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.vocab\n",
    "import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "available_gpu = torch.cuda.is_available()\n",
    "if available_gpu:\n",
    "  print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "  print(\"GPU not available.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting seeds for reproducibility"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reverse = False  # If True, source tokens are used in reverse for training: Consider (Sutskever et al. 2014)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting paths for data preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# File names from BEA 2019\n",
    "\n",
    "A_train = \"A.train.gold.bea19.m2\"\n",
    "B_train = \"B.train.gold.bea19.m2\"\n",
    "C_train = \"C.train.gold.bea19.m2\"\n",
    "\n",
    "A_dev = \"A.dev.gold.bea19.m2\"\n",
    "B_dev = \"B.dev.gold.bea19.m2\"\n",
    "C_dev = \"C.dev.gold.bea19.m2\"\n",
    "\n",
    "training_raw_data = [A_train, B_train, C_train]\n",
    "validation_raw_data = [A_dev, B_dev, C_dev]\n",
    "\n",
    "N_test = \"N.dev.gold.bea19.m2\"\n",
    "\n",
    "# Path to data of current project\n",
    "\n",
    "hard_path = \"data/m2/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing training data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessor_training = DataPreprocessor()\n",
    "\n",
    "for file in training_raw_data:\n",
    "  preprocessor_training.read_m2_data(hard_path + file)\n",
    "\n",
    "print(len(preprocessor_training.data))\n",
    "print(preprocessor_training.data[15:20])\n",
    "\n",
    "train_data = preprocessor_training.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enlarge training data with more examples of correct sentences."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_only_correct = []\n",
    "\n",
    "for element in train_data:\n",
    "  element[\"tgt\"]\n",
    "\n",
    "  train_data_only_correct.append({\"src\": element[\"tgt\"], \"tgt\": element[\"tgt\"]})\n",
    "\n",
    "print(len(train_data_only_correct))\n",
    "print(train_data_only_correct[15:20])\n",
    "\n",
    "train_data = train_data + train_data_only_correct\n",
    "\n",
    "print(len(train_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing validation data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessor_val = DataPreprocessor()\n",
    "\n",
    "for file in validation_raw_data:\n",
    "  preprocessor_val.read_m2_data(hard_path + file)\n",
    "\n",
    "print(len(preprocessor_val.data))\n",
    "print(preprocessor_val.data[5:10])\n",
    "\n",
    "val_data = preprocessor_val.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparing test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessor_test = DataPreprocessor()\n",
    "\n",
    "preprocessor_test.read_m2_data(hard_path + N_test)\n",
    "\n",
    "print(len(preprocessor_test.data))\n",
    "print(preprocessor_test.data[0:5])\n",
    "\n",
    "test_data = preprocessor_test.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trimming sentences and adding sos and eos tokens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trim_sentences(input_dict, max_length, lower, reverse, sos_token, eos_token):\n",
    "\n",
    "    src_tokens = [entry for entry in input_dict[\"src\"].split()][:max_length]\n",
    "    tgt_tokens = [entry for entry in input_dict[\"tgt\"].split()][:max_length]\n",
    "\n",
    "    if lower:\n",
    "        src_tokens = [token.lower() for token in src_tokens]\n",
    "        tgt_tokens = [token.lower() for token in tgt_tokens]\n",
    "\n",
    "    if reverse:  # Reversing source sentence\n",
    "      src_tokens = src_tokens[::-1]\n",
    "\n",
    "    src_tokens = [sos_token] + src_tokens + [eos_token]\n",
    "    tgt_tokens = [sos_token] + tgt_tokens + [eos_token]\n",
    "\n",
    "    input_dict[\"src_tokens\"] = src_tokens\n",
    "    input_dict[\"tgt_tokens\"] = tgt_tokens\n",
    "\n",
    "    return input_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 25\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "train_data = list(map(lambda x: trim_sentences(x, max_length, lower, reverse, sos_token,\n",
    "                                               eos_token), train_data))\n",
    "\n",
    "val_data = list(map(lambda x: trim_sentences(x, max_length, lower, reverse, sos_token,\n",
    "                                             eos_token), val_data))\n",
    "\n",
    "test_data = list(map(lambda x: trim_sentences(x, max_length, lower, reverse, sos_token,\n",
    "                                              eos_token), test_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[30]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vocabularies\n",
    "\n",
    "Each word will receive a number within the given vocabulary. Unknown words receive \"unk\" and are labeled with index 0.\n",
    "\n",
    "The vocabulary is enriched with special tokens: *unk_token*, *sos_token*, *eos_token*, and *\\<pad\\>*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merging entries from training data to build rich vodabulary\n",
    "train_data_src_list = [entry['src_tokens'] for entry in train_data]\n",
    "train_data_tgt_list = [entry['tgt_tokens'] for entry in train_data]\n",
    "train_data_list = train_data_src_list + train_data_tgt_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_freq = 3\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "  unk_token,\n",
    "  pad_token,\n",
    "  sos_token,\n",
    "  eos_token,\n",
    "]\n",
    "\n",
    "# Torch is building the vocabulary with indeces\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "  train_data_list,\n",
    "  min_freq=min_freq,\n",
    "  specials=special_tokens,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_vocab.get_itos()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_vocab.get_itos()[9]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_vocab.get_stoi()[\"the\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_vocab[\"the\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(en_vocab) # With larger training data, e.g. by adding correct examples, the vocab increases -> because min_freq is now met more often"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Torch text does not automatically assign a value to *unk* and other specific tokens. Therefore, if a word is unknown so far no numerical value is returned."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]\n",
    "en_vocab.set_default_index(unk_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"The\" in en_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_vocab[\"The\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = [\"i\", \"love\", \"watching\", \"crime\", \"shows\"]\n",
    "en_vocab.lookup_indices(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def numericalize_example(input_dict, en_vocab):\n",
    "\n",
    "  src_ids = en_vocab.lookup_indices(input_dict[\"src_tokens\"])\n",
    "  tgt_ids = en_vocab.lookup_indices(input_dict[\"tgt_tokens\"])\n",
    "\n",
    "  # Token ids are saved as torch tensors: See type(train_data[0][\"src_ids\"])\n",
    "  input_dict[\"src_ids\"] = torch.tensor(src_ids, dtype=torch.int64)\n",
    "  input_dict[\"tgt_ids\"] = torch.tensor(tgt_ids, dtype=torch.int64)\n",
    "\n",
    "  return input_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = list(map(lambda x: numericalize_example(x, en_vocab), train_data))\n",
    "val_data = list(map(lambda x: numericalize_example(x, en_vocab), val_data))\n",
    "test_data = list(map(lambda x: numericalize_example(x, en_vocab), test_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(train_data[0][\"src_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Data Loader\n",
    "\n",
    "Data loader helps to combine the training instances into batches. For successful batches, padding is needed so that each batch has equal size.\n",
    "\n",
    "\"The `get_collate_fn` takes in the padding token index and returns the `collate_fn` defined inside it. This technique, of defining a function inside another and returning it, is known as a [closure](<https://en.wikipedia.org/wiki/Closure_(computer_programming)>). It allows the `collate_fn` to continually use the value of `pad_index` it was created with without creating a class or using global variables.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "\n",
    "  def collate_fn(batch):\n",
    "\n",
    "    batch_src_ids = [example[\"src_ids\"] for example in batch]\n",
    "    batch_tgt_ids = [example[\"tgt_ids\"] for example in batch]\n",
    "    batch_src_ids = nn.utils.rnn.pad_sequence(batch_src_ids, padding_value=pad_index)\n",
    "    batch_tgt_ids = nn.utils.rnn.pad_sequence(batch_tgt_ids, padding_value=pad_index)\n",
    "\n",
    "    batch = {\n",
    "      \"src_ids\": batch_src_ids,\n",
    "      \"tgt_ids\": batch_tgt_ids,\n",
    "    }\n",
    "\n",
    "    return batch\n",
    "\n",
    "  return collate_fn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question: What happens when batches are being shuffled here? -- they are shuffled for training, but not for the validation and test sets.\n",
    "\n",
    "Shuffling the training data can improve the stability of the training process and potentially enhance the model's overall performance. However, this step is only required for the training set. The metrics for the validation and test sets will remain consistent, regardless of the data order."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "\n",
    "  collate_fn = get_collate_fn(pad_index)\n",
    "\n",
    "  data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=shuffle,\n",
    "  ) # Try \"pin_memory = True\" / \"num_workers = 2\"\n",
    "\n",
    "  return data_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Batch size should correspond to memory of GPU.\n",
    "\n",
    "Question: How to find out about the current memory?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "val_data_loader = get_data_loader(val_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#The model\n",
    "\n",
    "The model has three parts: Encoder, Decoder, and seq2seq. All shall be implemented separately."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##1) Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "\n",
    "    super().__init__() # Inherit boilerplate code from Torch\n",
    "\n",
    "    self.hidden_dim = hidden_dim  # Dimension of one-hot vectors\n",
    "    self.n_layers = n_layers  # Number of layers in LSTM network\n",
    "    self.embedding = nn.Embedding(input_dim, embedding_dim)  # Converts one-hot vectors into lower dimension embeddings\n",
    "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    self.dropout = nn.Dropout(dropout)  # Regularization parameter\n",
    "\n",
    "  def forward(self, src):  # src = [src length, batch size]\n",
    "\n",
    "    embedded = self.dropout(self.embedding(src)) # embedded = [src length, batch size, embedding dim]\n",
    "\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    # outputs = [src length, batch size, hidden dim * n directions]\n",
    "    # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "    # cell = [n layers * n directions, batch size, hidden dim]\n",
    "    # outputs are always from the top hidden layer\n",
    "\n",
    "    return hidden, cell"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##2) Decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.output_dim = output_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "    self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    self.fc_out = nn.Linear(hidden_dim, output_dim)  # Additional output layer for predictions\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, input, hidden, cell):\n",
    "    # input = [batch size]\n",
    "    # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "    # cell = [n layers * n directions, batch size, hidden dim]\n",
    "    # n directions in the decoder will both always be 1, therefore:\n",
    "    # hidden = [n layers, batch size, hidden dim]\n",
    "    # context = [n layers, batch size, hidden dim]\n",
    "\n",
    "    input = input.unsqueeze(0)  # input = [1, batch size]\n",
    "\n",
    "    embedded = self.dropout(self.embedding(input))  # embedded = [1, batch size, embedding dim]\n",
    "\n",
    "    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))  # output = [seq length, batch size, hidden dim * n directions]\n",
    "\n",
    "    # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "    # cell = [n layers * n directions, batch size, hidden dim]\n",
    "    # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "    # output = [1, batch size, hidden dim]\n",
    "    # hidden = [n layers, batch size, hidden dim]\n",
    "    # cell = [n layers, batch size, hidden dim]\n",
    "\n",
    "    prediction = self.fc_out(output.squeeze(0))  # prediction = [batch size, output dim]\n",
    "\n",
    "    return prediction, hidden, cell"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##3) seq2seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder, device):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.device = device  # GPU if available\n",
    "\n",
    "    assert (\n",
    "      encoder.hidden_dim == decoder.hidden_dim\n",
    "    ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "\n",
    "    assert (\n",
    "      encoder.n_layers == decoder.n_layers\n",
    "    ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "  def forward(self, src, tgt, teacher_forcing_ratio):\n",
    "    # src = [src length, batch size]\n",
    "    # tgt = [tgt length, batch size]\n",
    "    # teacher_forcing_ratio is probability to use teacher forcing\n",
    "    # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "\n",
    "    batch_size = tgt.shape[1]\n",
    "    tgt_length = tgt.shape[0]\n",
    "    tgt_vocab_size = self.decoder.output_dim\n",
    "\n",
    "    # tensor to store decoder outputs\n",
    "    outputs = torch.zeros(tgt_length, batch_size, tgt_vocab_size).to(self.device)\n",
    "\n",
    "    # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "    hidden, cell = self.encoder(src) # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "    # cell = [n layers * n directions, batch size, hidden dim]\n",
    "    # first input to the decoder is the <sos> tokens\n",
    "\n",
    "    input = tgt[0, :]  # input = [batch size]\n",
    "\n",
    "    for t in range(1, tgt_length):\n",
    "      # insert input token embedding, previous hidden and previous cell states\n",
    "      # receive output tensor (predictions) and new hidden and cell states\n",
    "\n",
    "      output, hidden, cell = self.decoder(input, hidden, cell)  # output = [batch size, output dim]\n",
    "      # hidden = [n layers, batch size, hidden dim]\n",
    "      # cell = [n layers, batch size, hidden dim]\n",
    "      # place predictions in a tensor holding predictions for each token\n",
    "\n",
    "      outputs[t] = output\n",
    "\n",
    "      # decide if we are going to use teacher forcing or not\n",
    "      teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "      # get the highest predicted token from our predictions\n",
    "      top1 = output.argmax(1)\n",
    "\n",
    "      # if teacher forcing, use actual next token as next input\n",
    "      # if not, use predicted token\n",
    "      input = tgt[t] if teacher_force else top1  # input = [batch size]\n",
    "\n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiating the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_dim = len(en_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "\n",
    "encoder_embedding_dim = 256  # 256, 300, 1000\n",
    "decoder_embedding_dim = 256  # 256, 300, 1000\n",
    "\n",
    "hidden_dim = 512  # 512, 600, 2000 What does hidden dimensions? -- corresponds to the size of the hidden state of LSTM layer\n",
    "\n",
    "n_layers = 2  # 4?\n",
    "\n",
    "encoder_dropout = 0.5  # 0.25, 0.5, 0.75 It should be 0.5 (Raschka et al. 2022)\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "  input_dim,\n",
    "  encoder_embedding_dim,\n",
    "  hidden_dim,\n",
    "  n_layers,\n",
    "  encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "  output_dim,\n",
    "  decoder_embedding_dim,\n",
    "  hidden_dim,\n",
    "  n_layers,\n",
    "  decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weights\n",
    "\n",
    "\"In the paper they state they initialize all weights from a uniform distribution between -0.08 and +0.08, i.e. $\\mathcal{U}(-0.08, 0.08)$.\" (Trevett 2024)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "\n",
    "  for name, param in m.named_parameters():\n",
    "\n",
    "    # Samples are taken form a uniform distribution, this corresponds to values ranging between [-0.08, 0.08]\n",
    "    nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Loss function\n",
    "\n",
    "\"Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token.\" (Trevett 2024)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Training loop\n",
    "\n",
    "\"At each iteration:\n",
    "\n",
    "-   get the source and target sentences from the batch, $X$ and $Y$\n",
    "-   zero the gradients calculated from the last batch\n",
    "-   feed the source and target into the model to get the output, $\\hat{Y}$\n",
    "-   as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with `.view`\n",
    "    -   we slice off the first column of the output and target tensors as mentioned above\n",
    "-   calculate the gradients with `loss.backward()`\n",
    "-   clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "-   update the parameters of our model by doing an optimizer step\n",
    "-   sum the loss value to a running total\" (Trevett 2024)\n",
    "\n",
    "Clipping might not be necessarily needed with the LSTM; but experiences appear to differ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model,\n",
    "    data_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    clip,\n",
    "    teacher_forcing_ratio,\n",
    "    device\n",
    "):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for i, batch in enumerate(data_loader):\n",
    "\n",
    "    src = batch[\"src_ids\"].to(device)  # src = [src length, batch size]\n",
    "    tgt = batch[\"tgt_ids\"].to(device)  # tgt = [tgt length, batch size]\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()  # Zero out gradients in the optimizer form previous step\n",
    "\n",
    "    output = model(src, tgt, teacher_forcing_ratio)  # output = [tgt length, batch size, tgt vocab size] as predictions\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output[1:].view(-1, output_dim)  # output = [(tgt length - 1) * batch size, tgt vocab size]\n",
    "\n",
    "    tgt = tgt[1:].view(-1)  # tgt = [(tgt length - 1) * batch size]\n",
    "\n",
    "    loss = criterion(output, tgt)  # Calculate loss\n",
    "    loss.backward()  # Compute gradients\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Clipping against exploding gradients\n",
    "\n",
    "    optimizer.step()  # Update parameters using gradients\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss / len(data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Evaluation loop\n",
    "\n",
    "\"Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value.\n",
    "\n",
    "We must remember to set the model to evaluation mode with `model.eval()`. This will turn off dropout (and batch normalization, if used).\n",
    "\n",
    "We use the `with torch.no_grad()` block to ensure no gradients are calculated within the block. This reduces memory consumption and speeds things up.\" (Trevett 2024)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "\n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  with torch.no_grad(): # Context-manager that disables gradient calculation: Helps with inferencing.\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "\n",
    "      src = batch[\"src_ids\"].to(device)  # src = [src length, batch size]\n",
    "      tgt = batch[\"tgt_ids\"].to(device)  # tgt = [tgt length, batch size]\n",
    "\n",
    "      output = model(src, tgt, 0)  # turn off teacher forcing\n",
    "      # output = [tgt length, batch size, tgt vocab size]\n",
    "\n",
    "      output_dim = output.shape[-1]\n",
    "      output = output[1:].view(-1, output_dim)  # output = [(tgt length - 1) * batch size, tgt vocab size]\n",
    "\n",
    "      tgt = tgt[1:].view(-1)  # tgt = [(tgt length - 1) * batch size]\n",
    "\n",
    "      loss = criterion(output, tgt)\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss / len(data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Model training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "train_loss_history = [0] * n_epochs\n",
    "valid_loss_history = [0] * n_epochs\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "\n",
    "  train_loss = train_fn(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    clip,\n",
    "    teacher_forcing_ratio,\n",
    "    device,\n",
    "  )\n",
    "\n",
    "  valid_loss = evaluate_fn(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    criterion,\n",
    "    device,\n",
    "  )\n",
    "\n",
    "  train_loss_history[epoch] = train_loss\n",
    "  valid_loss_history[epoch] = valid_loss\n",
    "\n",
    "  if valid_loss < best_valid_loss:\n",
    "    best_valid_loss = valid_loss\n",
    "    torch.save(model.state_dict(), \"gec-model_4_large.pt\") # Save model with weights as dict\n",
    "\n",
    "  print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "  print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_arr = np.arange(len(train_loss_history)) + 1\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, train_loss_history, \"-o\", label=\"Train loss\")\n",
    "ax.plot(x_arr, valid_loss_history, \"--<\", label=\"Valid loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend(fontsize=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Evaluation\n",
    "\n",
    "Loss here is derived from Cross Entropy (consider 'criterion' from previous cell). Perplexity is a metric that indicates how much \"surprised\" a model is when \"seeing\" new data. Perplexity is computed as the exponentiation of the loss produced by the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "  model.load_state_dict(torch.load(\"gec-model_4_large.pt\"))\n",
    "else:\n",
    "  model.load_state_dict(torch.load(\"gec-model_4_large.pt\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for making inferences with the model\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def correct_sentence(\n",
    "  sentence,\n",
    "  model,\n",
    "  en_vocab,\n",
    "  lower,\n",
    "  reverse,\n",
    "  sos_token,\n",
    "  eos_token,\n",
    "  device,\n",
    "  max_output_length=25,\n",
    "):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "\n",
    "      if reverse:\n",
    "        tokens = sentence.split()[::-1]\n",
    "      else:\n",
    "        tokens = sentence.split()\n",
    "\n",
    "    else:\n",
    "\n",
    "      if reverse:\n",
    "        tokens = [token for token in sentence][::-1]\n",
    "      else:\n",
    "        tokens = [token for token in sentence]\n",
    "\n",
    "    if lower:\n",
    "\n",
    "      tokens = [token.lower() for token in tokens]\n",
    "      tokens = [sos_token] + tokens + [eos_token]\n",
    "      ids = en_vocab.lookup_indices(tokens)\n",
    "      tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "      hidden, cell = model.encoder(tensor)\n",
    "      inputs = en_vocab.lookup_indices([sos_token])\n",
    "\n",
    "      for _ in range(max_output_length):\n",
    "\n",
    "        inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "        output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "        predicted_token = output.argmax(-1).item()\n",
    "        inputs.append(predicted_token)\n",
    "\n",
    "        if predicted_token == en_vocab[eos_token]:\n",
    "\n",
    "            break\n",
    "\n",
    "        tokens = en_vocab.lookup_tokens(inputs)\n",
    "\n",
    "  return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence = test_data[2][\"src\"]\n",
    "expected_correction = test_data[2][\"tgt\"]\n",
    "\n",
    "sentence, expected_correction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correction = correct_sentence(\n",
    "  sentence,\n",
    "  model,\n",
    "  en_vocab,\n",
    "  lower,\n",
    "  reverse,\n",
    "  sos_token,\n",
    "  eos_token,\n",
    "  device\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Inference\n",
    "\n",
    "The trained model is fed with the test data. The predictions with source and target sentences are stored at `test/model_name/`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}